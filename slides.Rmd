---
title: "Trabajo Práctico – IECD"
author: "Grupo 11"
date: "2025-12-8"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

# Introducción

## Objetivo del trabajo

Análisis de propiedades estadísticas de tests diagnósticos:

1. **Test perfecto** (baseline)
2. **Test imperfecto** con $S_e$ y $S_p$ conocidos  
3. **Dos muestras** (pre-post intervención)

---

# Parte I: Test perfecto (baseline)

## 1.1 Distribución de $T_{per}$

### Contexto
- Población con prevalencia $\theta = P(Y=1)$
- Test perfecto: $Se = Sp = 1$
- $Y_i \sim \text{Bernoulli}(\theta)$, i.i.d.

### Variable de interés
$$T_{per} = \sum_{i=1}^n Y_i$$

### Distribución
$$T_{per} \sim \text{Binomial}(n,\theta)$$

$$\boxed{P(T_{per} = k) = \binom{n}{k}\theta^{k}(1-\theta)^{n-k}}$$

---

## 1.2 Estimador de Máxima Verosimilitud

### Función de verosimilitud
$$L(\theta) = \prod_{i=1}^n \theta^{Y_i}(1-\theta)^{1-Y_i} = (1-\theta)^{N_0}\,\theta^{N_1}$$

### Log-verosimilitud
$$\ell(\theta) = N_0 \ln(1-\theta) + N_1 \ln(\theta)$$

### Estimador EMV
$$\hat{\theta}_{per} = \frac{N_1}{n} = \frac{T_{per}}{n}$$

---

## 1.3 Sesgo, Varianza, Error cuadrático medio, consistencia y distribución asintótica

* **Sesgo:**  
  El estimador \(\hat{\theta}_{per} = \frac{T_{\text{per}}}{n}\) es insesgado porque
  \[
  \mathbb{E}[\hat{\theta}_{per}]
    = \mathbb{E}\left[\frac{T_{\text{per}}}{n}\right]
    = \frac{1}{n}\mathbb{E}[T_{\text{per}}]
    = \frac{n\theta}{n}
    = \theta.
  \]
  Usamos que \(T_{\text{per}} \sim \mathrm{Binomial}(n,\theta)\), por lo que  \(\mathbb{E}[T_{\text{per}}] = n\theta\).

---

* **Varianza:**  
  \[
  \operatorname{Var}(\hat{\theta}_{per})
    = \operatorname{Var}\left(\frac{T_{\text{per}}}{n}\right)
    = \frac{1}{n^2}\operatorname{Var}(T_{\text{per}})
    = \frac{n\theta(1-\theta)}{n^2}
    = \frac{\theta(1-\theta)}{n}.
  \]
  También utilizamos que \(T_{\text{per}}\) es binomial.
  
\vspace{0.5cm}

* **Error cuadrático medio (ECM):**  
  Como el sesgo es cero,
  \[
  \text{ECM}(\hat{\theta}_{per})
    = \operatorname{Var}(\hat{\theta}_{per})
    + \mathbb{B}^2(\hat{\theta}_{per})
    = \frac{\theta(1-\theta)}{n}.
  \]

---

* **Consistencia:**  
  Observamos que \(\hat{\theta}_{per} = \frac{T_{\text{per}}}{n} = \frac{1}{n}\sum_{i=1}^n Y_i\),  
  donde \(\mathbb{E}[Y_i] = \theta\) y \(\operatorname{Var}(Y_i)=\theta(1-\theta)\).  
  Por la Ley Fuerte de los Grandes Números,
  \[
  \hat{\theta}_{per} \xrightarrow{c.s.} \theta.
  \]
  Por lo tanto, \(\hat{\theta}_{per}\) es **fuertemente consistente**.

\vspace{0.5cm}

* **Distribución asintótica:**  
  Por el Teorema Central del Límite,
  \[
  \sqrt{n}\,(\hat{\theta}_{per} - \theta)
  \xrightarrow{d} N\bigl(0,\theta(1-\theta)\bigr).
  \]

---

## 1.4 Intervalo de confianza para $\theta$

Sabemos que por TCL:

\[
\sqrt{n}\,\frac{(\hat{\theta}_{per} - \theta)}{\sqrt{\theta(1-\theta)}}
\xrightarrow{d} N\bigl(0,1\bigr).
\]

Además, por ley débil de los grandes números:

\[
\hat{\theta}_{per} \xrightarrow{p} \theta
\]

---

Entonces, combinando los resultados anteriores con Slutsky:

\[
\sqrt{n}\,\frac{(\hat{\theta}_{per} - \theta)}{\sqrt{\hat{\theta}_{per}(1-\hat{\theta}_{per})}}
\xrightarrow{d} N\bigl(0,1\bigr).
\]

Como $P(Z\ge1.96) = 0.025$, vale lo siguiente:

\[
P\!\left(-1.96 \le \sqrt{n}\,\frac{\hat{\theta}_{per} - \theta}{\sqrt{\hat{\theta}_{per}(1-\hat{\theta}_{per})}} \le 1.96\right)
\xrightarrow[n\to\infty]{} 0.95.
\]

---

Finalmente, se deduce el siguiente intervalo de nivel asintótico 0.95:

\[
\boxed{IC_{0.95}(\theta) = \hat{\theta}_{per} \pm 1.96\sqrt{\frac{\hat{\theta}_{per}(1-\hat{\theta}_{per})}{n}}}
\]

---

## 1.5 Cubrimiento empírico

Para evaluar el cubrimiento empírico del intervalo dado en $1.4$, se llevó a cabo una simulación Monte Carlo para los siguientes valores de n:

\[
10, 20, 50, 100, 1000, 10000
\]


```{r, echo=FALSE, eval=TRUE, size="tiny"}
#fijamos la semilla que se va a utilizar durante todo el tp.
set.seed(43)

tita <- 0.25
ns <- c(10, 20, 50, 100, 1000, 10000) #valores de n para los cuales vamos a evaluar la cobertura
Nrep <- 5000                          #empirica
z <- qnorm(0.975)

cubrimientoN <- 0

#para cada n:
for (j in seq_along(ns)){
  n <- ns[j]
  cubrimiento <- 0
  
  #repito el experimento 5000 veces.
  for(i in 1:Nrep){                                         
    muestra <- rbinom(1,n,tita)
    
    estTita <- muestra/n
    termino <- z * sqrt((estTita*(1-estTita))/n)
    intervalo <- c(estTita - termino, estTita + termino)
    
    cubrimiento[i] <- intervalo[1] <= tita && tita <= intervalo[2]
  }
  
  #promedio de veces que el valor real de tita cayó en el intervalo
  cubrimientoN[j] <- mean(cubrimiento)                  
}

cubrimientoEmpirico <- data.frame(n = ns, cubrimiento = cubrimientoN)
```
---

Y se obtuvieron estos resultados:

```{r, echo=FALSE, eval=TRUE}
knitr::kable(cubrimientoEmpirico, digits = 4, caption = "Cubrimiento empírico del intervalo")
```

---


Los resultados obtenidos son coherentes con la teoría. Al tratarse de un intervalo asintótico, el cubrimiento empírico converge hacia el nivel nominal del 0.95 conforme aumenta el tamaño muestral n.


---

# Parte II: Test imperfecto con $S_e$ y $S_p$ conocidos

## 2.0.1 Estimación de $p$ con $T_{per}$

### Estimador de $p$
$$\hat{p} = \frac{T_{per}}{n}$$

### Distribución
$$T_{per} \sim \text{Binomial}(n, p)$$

### Propiedades
- $\mathbb{E}[\hat{p}] = p$
- $\operatorname{Var}(\hat{p}) = \frac{p(1-p)}{n}$
- $\hat{p}$ es EMV de $p$

---

## 2.0.2 $p$ como función de $\theta$, $S_{e}$ y $S_{p}$.

\[
\boxed{p = Se\,\theta + (1-Sp)(1-\theta)}
\]

### Formas equivalentes
\[
p = (Se + Sp - 1)\,\theta + (1-Sp)
\]

\[
\theta = \frac{p + Sp - 1}{Se + Sp - 1}
\]

### Casos límite
\[
\theta = 0 \Rightarrow p = 1 - Sp
\]
\[
\theta = 1 \Rightarrow p = Se
\]

---

## 2.0.3 Comportamiento de $p = P(T=1)$

### Fórmula base
\[
p(\theta,Se,Sp) = Se\,\theta + (1-Sp)(1-\theta)
\]

### Valores de referencia
\[
Se = 0.9, \quad Sp = 0.95, \quad \theta = 0.25
\]
\[
p(0.25) = 0.2625
\]

---

## $p$ vs $\theta$ (fijos $Se$, $Sp$)

### Comportamiento
- **Lineal creciente** en $\theta$


### Límites
\[
\theta = 0 \Rightarrow p = 1 - Sp = 0.05
\]
\[
\theta = 1 \Rightarrow p = Se = 0.9
\]

### Interpretación
- **$\theta$ bajo**: $p$ dominada por falsos positivos
- **$\theta$ alto**: $p$ dominada por verdaderos positivos

---

## $p$ vs $Se$ (fijos $\theta$, $Sp$)

### Comportamiento
- **Lineal creciente** en $Se$
- Pendiente: $\theta = 0.25$

### Efecto
- **$Se$ baja**: Pocos verdaderos positivos
- **$Se$ alta**: Mejor detección → $p$ mayor

### Para $Se = 0.9$:
\[
p = 0.9 \times 0.25 + 0.05 \times 0.75 = 0.2625
\]

---

## $p$ vs $Sp$ (fijos $\theta$, $Se$)

### Comportamiento
- **Lineal decreciente** en $Sp$
- Pendiente: $-(1-\theta) = -0.75$

### Efecto
- **$Sp$ baja**: Muchos falsos positivos → $p$ alto
- **$Sp$ alta**: Menos falsos positivos → $p$ bajo

### Para $Sp = 0.95$:
\[
p = 0.225 + 0.05 \times 0.75 = 0.2625
\]

---

## Conclusiones clave

### Relaciones lineales
1. **$p \nearrow$ con $\theta$** 
2. **$p \nearrow$ con $Se$** 
3. **$p \searrow$ con $Sp$** 

---

## 2.1.4 Estimador de momentos (MoM)

Por item $2.0.2$, sabemos que:

\[
p = S_{e} \theta + (1-S_{p})(1-\theta)
\]

Reagrupando:
\[
p = (S_{e}+S_{p}-1)\theta + 1 - S_{p}
\]


Entonces:
\[
\theta = \frac{p + S_{p} - 1}{S_{e}+S_{p}-1}
\]

---

Como $\mathbb{E}[T_{i}] = p$, el estimador de momentos de $p$ es $\hat{p}_{MoM} = \frac{T_{per}}{n}$.

\vspace{0.5cm}

Finalmente, el estimador plug-in de momentos de $\theta$ es el siguiente:

\[
\boxed{\hat{\theta}_{MoM} = \frac{\hat{p}_{MoM} + S_{p} - 1}{S_{e}+S_{p}-1}}
\]

---

## 2.1.5 Sesgo, varianza y ECM

* **Sesgo**:

  Primero se observa que: 
  
  \[
  \mathbb{E}[\hat{p}_{MoM}] = \mathbb{E}\left[\frac{T_{\text{per}}}{n}\right]= p = (S_{e}+S_{p}-1)\theta + 1 - S_{p}
  \]
  
  Luego:
  
  \[
  \mathbb{E}[\hat{\theta}_{MoM}] = \mathbb{E}\left[\frac{\hat{p}_{MoM}+S_{p}-1}{S_{e}+S_{p}-1}\right] = \frac{\mathbb{E}[\hat{p}_{MoM}] + S_{p} - 1}{S_{e}+S_{p}-1} = \theta
  \]
  
  Por lo tanto $\hat{\theta}_{MoM}$ es un estimador insesgado de $\theta$.

---

* **Varianza**:

  Primero se observa que:

  \[
  Var[\hat{p}_{MoM}] = Var\left[\frac{T_{\text{per}}}{n}\right]=\frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}
  \]
  
  Luego:
  
  \[
  \operatorname{Var}[\hat{\theta}_{MoM}] = \operatorname{Var}\left[\frac{\hat{p}_{MoM}+S_{p}-1}{S_{e}+S_{p}-1}\right] = \frac{\operatorname{Var}[\hat{p}_{MoM} + S_{p} -1]}{(S_{e}+S_{p}-1)^2} =
  \]
  \[
  \frac{\operatorname{Var}[\hat{p}_{MoM}]}{(S_{e}+S_{p}-1)^2} = \frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}
\]

---

* **ECM**:
  
  Como el sesgo es 0:
  
  \[
  \text{ECM}(\hat{\theta}_{MoM})
    = \operatorname{Var}[\hat{\theta}_{MoM}]
    + \mathbb{B}^2[\hat{\theta}_{MoM}]
    =\frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}
  \]

---

* **Consistencia**
  
  Observemos que, por ley fuerte de los grandes números:
  
  \[
  \hat{p}_{MoM} = \frac{T_{per}}{n} \xrightarrow{cs} p
  \]
  
  Luego, como $\hat{\theta}_{MoM}$ es una función continua de $\hat{p}_{MoM}$:
  
  \[
  \hat{\theta}_{MoM} \xrightarrow{cs} \frac{p + S_{p}-1}{S_{e}+S_{p}-1} = \theta
  \]
  
  Por lo tanto, $\hat{\theta}_{MoM}$ es fuertemente consistente.

---

## 2.1.6 Comparación ECM: Test perfecto vs imperfecto

### Fórmulas
\[
\text{ECM}_{\text{perfecto}} = \frac{\theta(1-\theta)}{n}
\]
\[
\text{ECM}_{\text{imperfecto}} = \frac{p(1-p)}{n(S_e+S_p-1)^2}
\]

### Factor de aumento
\[
\boxed{\frac{1}{(S_e+S_p-1)^2}}
\]

### Para $S_e=0.9$, $S_p=0.95$
\[
S_e+S_p-1 = 0.85 \Rightarrow \frac{1}{0.85^2} \approx 1.38
\]
**ECM 38% mayor** con test imperfecto

---

## Comportamiento en función de $n$

```{r, echo=FALSE, fig.height=4.5}
tita <- 0.25
Se <- 0.9
Sp <- 0.95
p <- (Se + Sp - 1)*tita + (1 - Sp)
ns <- 10:1000

ECM_perfecto <- tita*(1 - tita) / ns
ECM_imperfecto <- p*(1 - p) / (ns * (Se + Sp - 1)^2)

plot(ns, ECM_imperfecto, type="l", col="orange", lwd=2,
     xlab="Tamaño muestral (n)", ylab="ECM",
     main="ECM vs n: Comparación entre tests")
lines(ns, ECM_perfecto, col="blue", lwd=2)

legend("topright", legend=c("Test imperfecto", "Test perfecto"),
       col=c("orange", "blue"), lwd=2, cex=0.9)

```

---

## Conclusiones clave

### Observaciones del gráfico
1. **ECM imperfecto > ECM perfecto** para todo $n$
2. Ambas **decrecen** como $1/n$

---

## 2.1.7 Validación: Teórico vs Simulado

### Diseño de simulación
- $R = 10000$ réplicas
- $n \in \{10, 20, 50, 100, 200\}$
- $\theta = 0.25$, $S_e = 0.9$, $S_p = 0.95$
- Estimador: $\hat{\theta}_{MoM}$

### Valores teóricos
\[
\operatorname{Var}_{\text{teórica}} = \frac{p(1-p)}{n(S_e+S_p-1)^2}
\]
\[
\text{ECM}_{\text{teórico}} = \operatorname{Var}_{\text{teórica}} \quad (\text{sesgo}=0)
\]

---

## Sesgo: Simulado vs Teórico (=0)

```{r, echo=FALSE, fig.height=4}
# Código adaptado para mostrar solo el gráfico
tita <- 0.25
Se <- 0.9
Sp <- 0.95
ns <- c(10, 20, 50, 100, 200)
R <- 10000

resultados <- data.frame(n = ns, bias_sim = NA)

for (i in seq_along(ns)) {
  n <- ns[i]
  p_Y <- Se*tita + (1 - Sp)*(1 - tita)
  est_imp <- numeric(R)
  
  for (r in 1:R) {
    T <- rbinom(n, 1, p_Y)
    p_hat <- mean(T)
    est_imp[r] <- (p_hat - (1 - Sp)) / (Se + Sp - 1)
  }
  resultados$bias_sim[i] <- mean(est_imp) - tita
}

plot(resultados$n, resultados$bias_sim, type="b", pch=19,
     xlab="n", ylab="Sesgo",
     main="Sesgo: Simulado vs Teórico",
     ylim=range(resultados$bias_sim))
abline(h = 0, col="blue", lwd=2, lty=2)
legend("topright", legend=c("Sesgo sim.", "Sesgo teórico (0)"),
       col=c("black","blue"), pch=c(19, NA), lty=c(1,2))
```

---

## Varianza: Simulado vs Teórica

```{r, echo=FALSE, fig.height=4}
# Cálculos para varianza
var_teo <- numeric(length(ns))
var_sim <- numeric(length(ns))

for (i in seq_along(ns)) {
  n <- ns[i]
  p_Y <- Se*tita + (1 - Sp)*(1 - tita)
  est_imp <- numeric(R)
  
  for (r in 1:R) {
    T <- rbinom(n, 1, p_Y)
    p_hat <- mean(T)
    est_imp[r] <- (p_hat - (1 - Sp)) / (Se + Sp - 1)
  }
  var_sim[i] <- var(est_imp)
  var_teo[i] <- (p_Y*(1 - p_Y)) / (n * (Se + Sp - 1)^2)
}

plot(ns, var_sim, type="b", pch=19, xlab="n", ylab="Varianza",
     main="Varianza: Simulado vs Teórica",
     ylim=c(0, max(var_sim, var_teo)))
lines(ns, var_teo, type="b", pch=17, col="blue")
legend("topright", legend=c("Var sim.", "Var teórica"),
       col=c("black","blue"), pch=c(19,17))
```

---

## ECM: Simulado vs Teórico

```{r, echo=FALSE, fig.height=4}
# Cálculos para ECM
ecm_sim <- numeric(length(ns))
ecm_teo <- var_teo  # insesgado

plot(ns, ecm_sim, type="b", pch=19, xlab="n", ylab="ECM",
     main="ECM: Simulado vs Teórico",
     ylim=c(0, max(ecm_sim, ecm_teo)))
lines(ns, ecm_teo, type="b", pch=17, col="blue")
legend("topright", legend=c("ECM sim.", "ECM teórico"),
       col=c("black","blue"), pch=c(19,17))
```


---

## Conclusiones de la validación

### Confirmaciones
1. **Sesgo aprox 0** en simulación → Insesgabilidad verificada
2. **Varianza simulada aprox Varianza teórica** para todo $n$
3. **ECM simulada aprox ECM teórico** → Fórmulas correctas

### Comportamiento observado
- **Convergencia**: A mayor $n$, mejor ajuste teórico-simulado
- **Consistencia**: Var/ECM decrecen como $1/n$
- **Robustez**: Fórmulas teóricas válidas aún para $n$ pequeño

### Implicaciones
- **Expresiones teóricas** son confiables
- **Estimador MoM** se comporta según teoría
- **Simulación** valida aproximaciones asintóticas

---

## 2.1.8 Bootstrap para $\hat{\theta}_{MoM}$ ($n=10$)

### Parámetros
- $\theta = 0.25$, $S_e = 0.9$, $S_p = 0.95$
- $n = 10$, $B = 5000$ réplicas bootstrap
- Bootstrap no paramétrico

### Método
1. Muestra original: $T_i \sim \text{Bernoulli}(p)$
2. Remuestreo con reemplazo
3. Recalcular $\hat{\theta}_{MoM}$ en cada réplica

---

## Distribución bootstrap ($n=10$)

```{r, echo=FALSE, fig.height=4.5}
tita <- 0.25
Se <- 0.9
Sp <- 0.95
n <- 10
B <- 5000

# Probabilidad real
p_Y <- Se * tita + (1 - Sp) * (1 - tita)

# Muestra original
set.seed(123)  # Para reproducibilidad
T_original <- rbinom(n, 1, p_Y)

# Bootstrap
boot_est <- numeric(B)
for (b in 1:B) {
  T_boot <- sample(T_original, size = n, replace = TRUE)
  p_hat_boot <- mean(T_boot)
  boot_est[b] <- (p_hat_boot - (1 - Sp)) / (Se + Sp - 1)
}

# Histograma
hist(boot_est, breaks = 15, freq = TRUE, col = "lightblue",
     main = "Distribución bootstrap - n = 10",
     xlab = expression(hat(theta)["MoM"]), 
     ylab = "Frecuencia", border = "darkblue")
abline(v = tita, col = "red", lwd = 3, lty = 2)
abline(v = mean(boot_est), col = "darkgreen", lwd = 2, lty = 2)
legend("topright", legend = c(expression(theta*" verdadero"), "Media bootstrap"),
       col = c("red", "darkgreen"), lwd = c(3, 2), lty = c(2, 2), cex = 0.8)
```

---

## Resultados bootstrap

### Estadísticas clave
\[
\text{Media bootstrap} = `r round(mean(boot_est), 4)`
\]
\[
\text{Sesgo bootstrap} = `r round(mean(boot_est) - tita, 4)`
\]
\[
\text{Desvío bootstrap} = `r round(sd(boot_est), 4)`
\]

### Características observadas
1. **Alta dispersión** (gran variabilidad)
2. **Sesgo positivo** 
3. **Distribución asimétrica** hacia derecha
4. **Valores fuera de [0,1]** posibles

---

## 2.2.9 Intervalos de confianza bootstrap

\vspace{0.5cm}

### Objetivo
Construir intervalos de confianza para $\theta$ basados en el estimador de momentos utilizando bootstrap no paramétrico.

### Método
- A partir de los datos ${T_i}$, se calcula $\hat\theta_{MoM}$ en cada remuestreo.

\vspace{0.2cm}

- Se generan $B = 1000$ réplicas bootstrap:  
  $\hat\theta^{*(b)}_{MoM}$, $b = 1,\dots,B$.
  
\vspace{0.2cm}
  
- Intervalo bootstrap percentil:

\[
IC^{boot}_{0.95} = 
\big[\, Q_{0.025}(\hat\theta^{*}),\; Q_{0.975}(\hat\theta^{*}) \,\big].
\]

---

### Simulación Monte Carlo
- Replicaciones: $R = 700$.
- Tamaños muestrales: $n \in \{10, 20, 50, 100, 1000\}$.
- Para cada réplica se obtiene:
  - **Cubrimiento empírico**: $1\{\theta \in IC^{boot}\}$
  - **Longitud**: $IC^{up} - IC^{low}$

```{r, echo = FALSE}

tita_corregida <- function(p_hat, Se, Sp) {
  (p_hat - (1 - Sp)) / (Se + Sp - 1)
}

#Definimos los valores a utilizar.
tita <- 0.25
Se <- 0.9
Sp <- 0.95
p_Y <- Se * tita + (1 - Sp) * (1 - tita)

ns <-  c(10, 20, 50, 100, 1000)
R <- 700 #replicas Monte Carlo
B <- 1000 #remuestreos bootstrap

z <- qnorm(0.975)

res_list <- vector("list",length(ns))
names(res_list) <- as.character(ns)

#para cada n:
for (j in seq_along(ns)){
  n <- ns[j]
  cubrimientos <- logical(R)
  longitudes <- numeric(R)
  
  #repito el experimento R veces 
  for (r in 1:R){
     T <- rbinom(n,1,p_Y)
     boot_est <- numeric(B)
     
     #genero B muestras bootstrap.
     for (b in 1:B){
       T_boot <- sample(T, size = n, replace = TRUE)
       p_hat_boot <- mean(T_boot)
       boot_est[b] <- tita_corregida(p_hat_boot, Se, Sp)
     }
     
     lower <- quantile(boot_est, probs = 0.025, names = FALSE)
     upper <- quantile(boot_est, probs = 0.975, names = FALSE)
     
     cubrimientos[r] <- (lower <= tita) && (tita <= upper)
     longitudes[r] <- (upper - lower)
  }
  
  #resultados para cada n
  res_list[[j]] <- list(
    n = n,
    cubrimiento = mean(cubrimientos),
    longitud_media = mean(longitudes)
  )
   
}

res_df <- do.call(rbind, lapply(res_list, function(z){
  data.frame(n = z$n,
             cubrimiento = z$cubrimiento,
             longitud_media = z$longitud_media)
}))
rownames(res_df) <- NULL
```

---
  
## 2.2.10 Intervalo de confianza asintótico
  
Recordemos que: 
  
* $\hat{p}_{MoM} = \frac{T_{per}}{n}$
\vspace{0.1cm}
* $\mathbb{E}[\hat{p}_{MoM}] = p$
\vspace{0.1cm}
* $\operatorname{Var}[\hat{p}_{MoM}] = \frac{p(1-p)}{n}$
\vspace{0.1cm}
* $\hat{\theta}_{MoM} = \frac{\hat{p}_{MoM} + S_{p} - 1}{S_{e}+S_{p}-1}$
\vspace{0.1cm}
* $\mathbb{E}[\hat{\theta}_{MoM}] = \theta$
\vspace{0.1cm}
* $\operatorname{Var}[\hat{\theta}_{MoM}] = \frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}$
  
---
  
* Entonces por TCL:
  
\[
\sqrt{n} (\hat{p}_{MoM} - p) \xrightarrow{D}  N\bigl(0,(p(1-p))\bigr)
\]
  
Defino $g(x) = \frac{x+S_{p}-1}{S_{e}+S_{p}-1}$ y $g'(x) = \frac{1}{S_{e} + S_{p} -1}$. Notar que $g(x)$ es $C^1$.
  
\vspace{0.2cm}

* Por Método Delta:
  
\[
\sqrt{n}(g(\hat{p}_{MoM}) - g(p)) \xrightarrow{D} N\bigl(0, \frac{p(1-p)}{(S_{e}+S_{p}-1)^2}\bigr)
\]

---

Luego,
  
\[
\frac{\hat{\theta}_{MoM}-\theta}{\sqrt{\operatorname{Var}[\hat{\theta}_{MoM}]}} \xrightarrow{D} N\bigl(0,1\bigr)
\]
  
  
* Además $\hat{p}_{MoM} = \frac{T_{per}}{n} \xrightarrow{cs} p$, entonces: 
  
\[ 
\sqrt{\frac{\hat{p}_{MoM}(1-\hat{p}_{MoM})}{p(1-p)}} \xrightarrow{P} 1 
\]
  
debido a que $h(x)= \sqrt{\frac{x(1-x)}{p(1-p)}}$ es continua en $(0,1)$.
  
---
  
Si $\widehat{\operatorname{Var}}[\hat{\theta}_{MoM}] =\frac{\hat{p}_{MoM}(1-\hat{p}_{MoM})}{n(S_{e}+S_{p}-1)^2}$, usando teorema de Slutsky:
  
\[
\frac{\hat{\theta}_{MoM} - \theta}{\sqrt{\widehat{\operatorname{Var}}[\hat{\theta}_{MoM}]}} = \frac{\hat{\theta}_{MoM} - \theta}{\sqrt{\operatorname{Var}[\hat{\theta}_{MoM}]}\sqrt{\frac{\hat{p}_{MoM}(1-\hat{p}_{MoM})}{p(1-p)}}} \xrightarrow{D}  N\bigl(0,1\bigr)
\]
  
  
* Por lo tanto, se deduce el siguiente intervalo de nivel asintótico $0.95$:
  
\[
\boxed{IC^{\theta}_{0.95} = \hat{\theta}_{MoM}\pm 1.96\sqrt{\widehat{\operatorname{Var}}[\hat{\theta}_{MoM}]}}
\]
  
donde $\widehat{\operatorname{Var}}[\hat{\theta}_{MoM}] = \frac{\hat{p}_{MoM}(1-\hat{p}_{MoM})}{n(S_{e}+S_{p}-1)^2}$

```{r, echo=FALSE}
res_listAsint <- vector("list", length(ns))
names(res_listAsint) <- as.character(ns)

#para cada n:
for (j in seq_along(ns)){
  n <- ns[j]
  cubrimientos <- logical(R)
  longitudes <- numeric(R)
  
  #repito el experimento R veces.
  for (r in 1:R){
    T <- rbinom(n, 1, p_Y)
    
    p_hat <- mean(T)
    tita_hat <- tita_corregida(p_hat,Se,Sp)
    est_Var <- (p_hat*(1-p_hat))/(n*(Se+Sp-1)^2)
    
    termino <- z*sqrt(est_Var)
    lower <- tita_hat - termino
    upper <- tita_hat + termino
    
    cubrimientos[r] <- (lower <= tita) && (tita <= upper)
    longitudes[r] <- (upper - lower)
  }
  
  #resultados para cada n.
  res_listAsint[[j]] <- list(
    n = n,
    cubrimiento = mean(cubrimientos),
    longitud_media = mean(longitudes)
  )
}

resAsint_df <- do.call(rbind, lapply(res_listAsint, function(z){
  data.frame(n = z$n,
             cubrimiento = z$cubrimiento,
             longitud_media = z$longitud_media)
}))
rownames(resAsint_df) <- NULL
```

---

## 2.2.11 Comparación entre ambos intervalos

Luego de realizar las simulaciones pertinentes, comparemos los resultados obtenidos.

```{r, echo=FALSE, eval=TRUE}
knitr::kable(res_df, digits = 4, caption = "Bootstrap Percentil")
```

---

```{r, echo=FALSE, eval=TRUE}
knitr::kable(resAsint_df, digits = 4, caption = "Intervalos Asintóticos")
```

---

* El bootstrap parece ser más robusto en muestras pequeñas, lo cual es esperable porque no depende de aproximaciones normales ni del método delta.

\vspace{0.3cm}

* El método asintótico da intervalos ligeramente más largos.

\vspace{0.3cm}

* A medida que aumenta $n$ los resultados convergen a los valores esperados.

---

## 2.3.12 Comportamiento del estimador de momentos

### Definición
El estimador de momentos se obtiene de:
\[
p = Se\,\theta + (1-Sp)(1-\theta)
\]
y reemplazando \(p\) por \(\hat p = T_{\text{per}}/n\):
\[
\hat{\theta}_{MoM} =
\frac{\hat p + Sp - 1}{Se + Sp - 1}.
\]

### Observación clave
- Aunque \(\hat p \in [0,1]\), la transformación es **lineal**, por lo que  
  \(\hat\theta_{MoM}\) **puede salir de \([0,1]\)**.
- Esto ocurre cuando la muestra tiene demasiados o muy pocos positivos.

### Condiciones
\[
\hat\theta_{MoM} < 0
\quad\Longleftrightarrow\quad
\hat p < 1 - Sp,
\]
\[
\hat\theta_{MoM} > 1
\quad\Longleftrightarrow\quad
\hat p > Se.
\]

---

## Ejemplos numéricos

### Valores de referencia
\[
Se = 0.9, \quad Sp = 0.95,
\qquad
1 - Sp = 0.05, \quad Se + Sp - 1 = 0.85.
\]

### Ejemplo 1: \(\hat\theta_{MoM} < 0\)
Si no hay positivos en la muestra:
\[
\hat p = 0.
\]
Entonces:
\[
\hat\theta_{MoM}
= \frac{0 + 0.95 - 1}{0.85}
= \frac{-0.05}{0.85}
\approx -0.0588.
\]

### Ejemplo 2: \(\hat\theta_{MoM} > 1\)
Si la muestra da un valor extremo:
\[
\hat p = 0.95,
\]
entonces:
\[
\hat\theta_{MoM}
= \frac{0.95 + 0.95 - 1}{0.85}
= \frac{0.90}{0.85}
\approx 1.0588.
\]

### Conclusión
- El estimador puede quedar fuera del intervalo válido para una probabilidad.
- Motivación para introducir el **estimador truncado**.

---

## 2.3.13 Estudio del estimador truncado

### Motivación
El estimador de momentos puede tomar valores fuera de \([0,1]\).  
Para evitar valores imposibles se define el estimador **truncado**:

### Definición
\[
\hat\theta_{\text{trunc}} =
\begin{cases}
\hat\theta_{\text{MoM}}, & 0 \le \hat\theta_{\text{MoM}} \le 1,\\[6pt]
0, & \hat\theta_{\text{MoM}} < 0,\\[6pt]
1, & \hat\theta_{\text{MoM}} > 1.
\end{cases}
\]

### Interpretación
- Coincide con \(\hat\theta_{MoM}\) cuando este es válido.
- Recorta valores negativos a 0 y mayores a 1 a 1.
- Introduce **sesgo** para muestras pequeñas, pero evita valores inaceptables.

---

## Simulación Monte Carlo del estimador truncado

### Parámetros usados
- \(\theta = 0.25\), \(Se = 0.9\), \(Sp = 0.95\)
- Tamaños muestrales: \(n = 10,\,100,\,1000\)
- Réplicas: \(N_{\text{rep}} = 10000\)

### Para cada réplica:
1. Generar \(T \sim \text{Binomial}(n, p_{verdadera})\)
2. Calcular \(\hat\theta_{\text{trunc}}\)
3. Estimar:
   - media  
   - sesgo  
   - varianza  
   - ECM = Sesgo\(^2\) + Var

### Tabla de resultados
\begin{table}[H]
\centering
\begin{tabular}{ccccc}
\hline
$n$ & media & sesgo & var & ECM \\
\hline
10   & 0.2485 & -0.00149 & 0.02463 & 0.02463 \\
100  & 0.25005 & 0.000048 & 0.002685 & 0.002685 \\
1000 & 0.25007 & 0.000067 & 0.000263 & 0.000263 \\
\hline
\end{tabular}
\end{table}

---

## Análisis del estimador truncado

### Sesgo
- Para \(n = 10\), el estimador muestra sesgo apreciable.  
  Esto ocurre porque \(\hat\theta_{MoM}\) cae a menudo por debajo de 0 y se trunca.
- El sesgo → 0 cuando \(n\) aumenta (coincide con el estimador original).

### Varianza
- Disminuye al crecer \(n\).
- Para \(n = 10\) es elevada por la alta variabilidad de \(\hat p\).
- Para \(n = 1000\) es muy baja y estable.

### ECM
\[
ECM = \text{Sesgo}^2 + \text{Var}.
\]
- Para muestras pequeñas, ECM grande por sesgo + varianza.
- Para muestras grandes, el ECM es muy pequeño.

### Conclusión
El estimador truncado:
- **es consistente**,  
- **es asintóticamente equivalente** al estimador de momentos,  
- pero **distorsiona muestras pequeñas** debido al truncamiento.

---

## Distribución del estimador truncado

```{r, echo = FALSE}
n_valores <- c(10, 100, 1000)
N_rep <- 10000
theta_verdadera <- 0.25
Se_fijo         <- 0.9
Sp_fijo         <- 0.95
p_verdadera <- Se_fijo * theta_verdadera + (1 - Sp_fijo) * (1 - theta_verdadera)

theta_mom <- function(T_obs, n, Se = Se_fijo, Sp = Sp_fijo) {
  p_muestral <- T_obs / n
  theta_mom_est <- (p_muestral + Sp - 1) / (Se + Sp - 1)
  return(theta_mom_est)
}

# Estimador truncado de theta
theta_trunc <- function(T_obs, n, Se = Se_fijo, Sp = Sp_fijo) {
  theta_mom_est <- theta_mom(T_obs, n, Se, Sp)
  theta_trunc_est <- pmin(1, pmax(0, theta_mom_est))
  return(theta_trunc_est)
}

par(mfrow = c(2, 2))  ## grid para los gráficos

for (n in n_valores) {
  # nueva data para cada n
  T_sim <- rbinom(N_rep, size = n, prob = p_verdadera)
  theta_trunc_sim <- theta_trunc(T_sim, n)
  
  # Histograma
  hist(theta_trunc_sim,
       breaks = 40,
       main = paste("Histograma de theta_trunc, n =", n),
       xlab = expression(hat(theta)[trunc]),
       probability = TRUE)
  
  media_hat <- mean(theta_trunc_sim)
  sd_hat    <- sd(theta_trunc_sim)
  x_grid    <- seq(0, 1, length.out = 200)
  lines(x_grid, dnorm(x_grid, mean = media_hat, sd = sd_hat), col = "red")
}

par(mfrow = c(1, 1))

n <- 1000
T_sim <- rbinom(N_rep, size = n, prob = p_verdadera)
theta_trunc_sim <- theta_trunc(T_sim, n)

qqnorm(theta_trunc_sim,
       main = expression(paste("QQ-plot de ", hat(theta)[trunc], " para n=1000")),
       xlab= "",
       ylab= "")
qqline(theta_trunc_sim)

```

---

### Observaciones de la simulación
- Para \(n = 10\):  
  - La distribución muestra **acumulación en 0 y 1**.  
  - No es aproximadamente normal.
- Para \(n = 100\):  
  - La masa en los bordes disminuye.  
  - La distribución empieza a ser unimodal y más suave.
- Para \(n = 1000\):  
  - El truncamiento ocurre con probabilidad casi nula.  
  - La distribución es bien aproximada por una normal:
\[
\hat\theta_{\text{trunc}} \approx
\mathcal{N}\left(
\theta,\;
\frac{p(1-p)}{n(Se+Sp-1)^2}
\right).
\]

### Conclusión general
- Para muestras grandes, el estimador truncado **preserva la distribución asintótica del MoM**.

---


