---
title: "Trabajo Práctico– IECD"
author: "Grupo 11"
date: "2025-12-8"
output: html_document
---

# Parte I: Test perfecto (baseline)

### 1.1 Observe $T_{per} \sim Bi(n,\theta)$
Consideremos una población en la que cada individuo puede estar o no enfermo. Sea la prevalencia verdadera de la enfermedad

$\hspace{7cm} \theta = P(Y = 1)$,

donde $Y$ es la variable aleatoria que indica el estado verdadero de una persona. Esto es, $Y = 1$ si la persona está enferma y $Y = 0$ si no lo está.

Se asume, por enunciado, que el test diagnóstico es perfecto, es decir, tanto la sensibilidad como la especificidad valen uno ($Se = Sp = 1$). En este caso, el resultado del test coincide exactamente con el estado verdadero del individuo.

Sea $Y_1, \dots, Y_n$ una muestra aleatoria de tamaño $n$ tomada de la población. Suponemos que las variables $Y_1, \dots, Y_n$ son independientes e idénticamente distribuidas, con


$\hspace{7cm} Y_i \sim Bernoulli(\theta)$ con $i = 1,\dots,n.$


Definimos la variable aleatoria que cuenta cuántas personas enfermas hay en la muestra como

$\hspace{7cm} T_{per} = \sum_{i=1}^n Y_i.$

Dado que $T_{per}$ es la suma de $n$ variables independientes con distribución Bernoulli$(\theta)$, se sigue que $T_{per}$ tiene una distribución binomial. En efecto, para $k = 0,1,\dots,n$ se cumple que

$\hspace{7cm} P(T_{per} = k)= \binom{n}{k}\theta^{k}(1-\theta)^{n-k}.$

Por lo tanto, concluimos que

$\hspace{7cm} T_{\text{per}} \sim \mathrm{Binomial}(n,\theta).$

$\hspace{16cm} \boxtimes$

### 1.2 Estimador de Máxima Verosimilitud de $\theta$

Como en el punto anterior, recordemos que bajo un test perfecto se tiene

$\hspace{7cm} T_{per} \sim Binomial(n,\theta),$

y que podemos representar los datos mediante variables Bernoulli

$\hspace{7cm} Y_1,\dots,Y_n \sim i.i.d. Bernoulli(\theta),$

donde $Y_i = 1$ si la persona $i$ está enferma y $Y_i = 0$ si no lo está.

Sea $N_1 = \sum_{i=1}^n Y_i$ la cantidad de individuos enfermos en la muestra y $N_0 = n - N_1$ la cantidad de individuos sanos.

La función de verosimilitud viene dada por

$\hspace{7cm} L(\theta)= \prod_{i=1}^n \theta^{Y_i}(1-\theta)^{1-Y_i}= (1-\theta)^{N_0}\,\theta^{N_1}.$

La log-verosimilitud resulta entonces

$\hspace{7cm} \ell(\theta) = \ln L(\theta)= N_0 \ln(1-\theta) + N_1 \ln(\theta).$


Buscamos $\theta \in (0,1)$ tal que

$\hspace{7cm} \frac{\partial \ell(\theta)}{\partial \theta} = 0.$

Derivando:

$\hspace{7cm} \frac{\partial \ell(\theta)}{\partial \theta}= \frac{N_0}{1-\theta}(-1) + \frac{N_1}{\theta}= -\frac{N_0}{1-\theta} + \frac{N_1}{\theta}.$

Igualamos a cero:

$\hspace{7cm} -\frac{N_0}{1-\theta} + \frac{N_1}{\theta} = 0 \quad \Longleftrightarrow \quad \frac{N_1}{\theta} = \frac{N_0}{1-\theta}.$

Despejando:

$\hspace{7cm} N_1(1-\theta) = N_0 \theta \quad \Longleftrightarrow \quad N_1 - N_1\theta = N_0\theta$

$\hspace{11cm} \Longleftrightarrow \quad N_1 = (N_0 + N_1)\theta = n\theta.$

Por lo tanto,

$\hspace{7cm} \hat{\theta}_{EMV} = \frac{N_1}{n}.$


Finalmente, como $N_1 = T_{per}$, obtenemos

$\hspace{7cm} \hat{\theta}_{per} = \frac{T_{per}}{n}.$


La segunda derivada confirma que se trata de un máximo, ya que

$\hspace{7cm} \frac{\partial^2 \ell(\theta)}{\partial \theta^2}= -\frac{N_0}{(1-\theta)^2} - \frac{N_1}{\theta^2} < 0, \forall \theta \in (0,1).$

$\hspace{16cm} \boxtimes$

### 1.3 Sesgo, Varianza, Error cuadrático medio, consistencia y distribución asintótica

* **Sesgo:**  
  El estimador \(\hat{\theta}_{per} = \frac{T_{\text{per}}}{n}\) es insesgado porque
  \[
  \mathbb{E}[\hat{\theta}_{per}]
    = \mathbb{E}\left[\frac{T_{\text{per}}}{n}\right]
    = \frac{1}{n}\mathbb{E}[T_{\text{per}}]
    = \frac{n\theta}{n}
    = \theta.
  \]
  Usamos que \(T_{\text{per}} \sim \mathrm{Binomial}(n,\theta)\), por lo que  \(\mathbb{E}[T_{\text{per}}] = n\theta\).

* **Varianza:**  
  \[
  \operatorname{Var}(\hat{\theta}_{per})
    = \operatorname{Var}\left(\frac{T_{\text{per}}}{n}\right)
    = \frac{1}{n^2}\operatorname{Var}(T_{\text{per}})
    = \frac{n\theta(1-\theta)}{n^2}
    = \frac{\theta(1-\theta)}{n}.
  \]
  También utilizamos que \(T_{\text{per}}\) es binomial.

* **Error cuadrático medio (ECM):**  
  Como el sesgo es cero,
  \[
  \text{ECM}(\hat{\theta}_{per})
    = \operatorname{Var}(\hat{\theta}_{per})
    + \mathbb{B}^2(\hat{\theta}_{per})
    = \frac{\theta(1-\theta)}{n}.
  \]

* **Consistencia:**  
  Observamos que \(\hat{\theta}_{per} = \frac{T_{\text{per}}}{n} = \frac{1}{n}\sum_{i=1}^n Y_i\),  donde \(\mathbb{E}[Y_i] = \theta\) y \(\operatorname{Var}(Y_i)=\theta(1-\theta)\).  
  Por la Ley Fuerte de los Grandes Números,
  \[
  \hat{\theta}_{per} \xrightarrow{c.s.} \theta.
  \]
  Por lo tanto, \(\hat{\theta}_{per}\) es **fuertemente consistente**.

* **Distribución asintótica:**  
  Por el Teorema Central del Límite,
  \[
  \sqrt{n}\,(\hat{\theta}_{per} - \theta)
  \xrightarrow{d} N\bigl(0,\theta(1-\theta)\bigr).
  \]


### 1.4 Intervalo de confianza para $\theta$.

Sabemos que por TCL:

\[
\sqrt{n}\,\frac{(\hat{\theta}_{per} - \theta)}{\sqrt{\theta(1-\theta)}}
\xrightarrow{d} N\bigl(0,1\bigr).
\]

Además, por ley débil de los grandes números:

\[
\hat{\theta}_{per} \xrightarrow{p} \theta
\]

Entonces, combinando los resultados anteriores con Slutsky:

\[
\sqrt{n}\,\frac{(\hat{\theta}_{per} - \theta)}{\sqrt{\hat{\theta}_{per}(1-\hat{\theta}_{per})}}
\xrightarrow{d} N\bigl(0,1\bigr).
\]

Como $P(Z\ge1.96) = 0.025$, vale lo siguiente:

\[
P\!\left(-1.96 \le \sqrt{n}\,\frac{\hat{\theta}_{per} - \theta}{\sqrt{\hat{\theta}_{per}(1-\hat{\theta}_{per})}} \le 1.96\right)
\xrightarrow[n\to\infty]{} 0.95.
\]


Finalmente, se deduce el siguiente intervalo de nivel asintótico 0.95:

\[
IC_{0.95}(\theta) = \hat{\theta}_{per} \pm 1.96\sqrt{\frac{\hat{\theta}_{per}(1-\hat{\theta}_{per})}{n}}
\]

<br>

### 1.5 Cubrimiento empírico.

Para evaluar el cubrimiento empírico del intervalo dado en $1.4$, se llevó a cabo una simulación Monte Carlo para los siguientes valores de n:

\[
10, 20, 50, 100, 1000, 10000
\]

```{r}
set.seed(43)

tita <- 0.25
ns <- c(10, 20, 50, 100, 1000, 10000) #valores de n para los cuales vamos a evaluar la cobertura
Nrep <- 5000                          #empirica
z <- qnorm(0.975)

cubrimientoN <- 0

for (j in seq_along(ns)){
  n <- ns[j]
  cubrimiento <- 0
  
  for(i in 1:Nrep){                                         #repito el experimento 5000 veces.
    muestra <- rbinom(1,n,tita)
    
    estTita <- muestra/n
    termino <- z * sqrt((estTita*(1-estTita))/n)
    intervalo <- c(estTita - termino, estTita + termino)
    
    cubrimiento[i] <- intervalo[1] <= tita && tita <= intervalo[2]
  }
  
  cubrimientoN[j] <- mean(cubrimiento)                  #promedio de veces que el valor real de 
                                                        #tita cayó en el intervalo.
}

cubrimientoEmpirico <- data.frame(n = ns, cubrimiento = cubrimientoN)
```

Y se obtuvieron estos resultados:

<div style="width: 50%; margin: auto;">

| n     | Cubrimiento |
|-------|-------------|
| 10    | 0.9232      |
| 20    | 0.8992      |
| 50    | 0.9362      |
| 100   | 0.9462      |
| 1000  | 0.9472      |
| 10000 | 0.9486      |

</div>

Los resultados obtenidos son coherentes con la teoría. Al tratarse de un intervalo asintótico, el cubrimiento empírico converge hacia el nivel nominal del 0.95 conforme aumenta el tamaño muestral
n.

<br><br>

# Parte II: Test imperfecto con $S_{e}$ y $S_{p}$ conocidos

### 2.0.1 Estimador $\hat{p}$ de $p$ con $T_{per}$
En esta parte definimos

$\hspace{7cm} p = P(T = 1),$

donde $T$ denota el resultado del test diagnóstico aplicado a un individuo: $T = 1$ si el test da positivo y $T = 0$ si el test da negativo. Suponemos que

$\hspace{7cm} T_1,\dots,T_n \sim i.i.d. Bernoulli(p),$

de modo que $P(T_i=1)=p$ y $P(T_i=0)=1-p$ para todo $i=1,\dots,n$.

Definimos la variable aleatoria

$\hspace{7cm} T_{per} = \sum_{i=1}^n T_i,$

que representa la cantidad de tests positivos observados en la muestra de tamaño $n$. Por ser suma de variables Bernoulli independientes con parámetro $p$, se tiene que

$\hspace{7cm} T_{per} \sim Binomial(n,p).$


Un estimador  para $p$ puede ser la proporción de individuos de la muestra cuyo test dio positivo. Por lo tanto, definimos

$\hspace{7cm} \hat p = \frac{T_{per}}{n}.$


Este estimador coincide además con el estimador de máxima verosimilitud (EMV) de $p$ en el modelo binomial, ya que la función de verosimilitud

$\hspace{7cm} L(p) \approx p^{T_{per}}(1-p)^{n-T_{per}}$

se maximiza en $p = T_{per}/n$.

<br>

### 2.1.4 Estimador de momentos (MoM).

Por item $2.0.2$, sabemos que:

\[
p = S_{e} \theta + (1-S_{p})(1-\theta)
\]

Reagrupando:
\[
p = (S_{e}+S_{p}-1)\theta + 1 - S_{p}
\]

Entonces:
\[
\theta = \frac{p + S_{p} - 1}{S_{e}+S_{p}-1}
\]


Como $\mathbb{E}[T_{i}] = p$, el estimador de momentos de $p$ es $\hat{p}_{MoM} = \frac{T_{per}}{n}$. Finalmente, el estiamdor plug-in de momentos de $\theta$ es el siguiente:

\[
\hat{\theta}_{MoM} = \frac{\hat{p}_{MoM} + S_{p} - 1}{S_{e}+S_{p}-1}
\]

<br>

### 2.1.5 Sesgo, varianza y ECM.

* **Sesgo**:

  Primero se observa que: 
  
  \[
  \mathbb{E}[\hat{p}_{MoM}] = \mathbb{E}\left[\frac{T_{\text{per}}}{n}\right]= p = (S_{e}+S_{p}-1)\theta + 1 - S_{p}
  \]
  
  Luego:
  
  \[
  \mathbb{E}[\hat{\theta}_{MoM}] = \mathbb{E}\left[\frac{\hat{p}_{MoM}+S_{p}-1}{S_{e}+S_{p}-1}\right] = \frac{\mathbb{E}[\hat{p}_{MoM}] + S_{p} - 1}{S_{e}+S_{p}-1} = \theta
  \]
  
  Por lo tanto $\hat{\theta}_{MoM}$ es un estimador insesgado de $\theta$.

* **Varianza**:

  Primero se observa que:

  \[
  Var[\hat{p}_{MoM}] = Var\left[\frac{T_{\text{per}}}{n}\right]=\frac{np(1-p)}{n^2} = \frac{p(1-p)}{n}
  \]
  
  Luego:
  
  \[
  \operatorname{Var}[\hat{\theta}_{MoM}] = \operatorname{Var}\left[\frac{\hat{p}_{MoM}+S_{p}-1}{S_{e}+S_{p}-1}\right] = \frac{\operatorname{Var}[\hat{p}_{MoM} + S_{p} -1]}{(S_{e}+S_{p}-1)^2} = \frac{\operatorname{Var}[\hat{p}_{MoM}]}{(S_{e}+S_{p}-1)^2} = \frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}
\]

* **ECM**:
  
  Como el sesgo es 0:
  
  \[
  \text{ECM}(\hat{\theta}_{MoM})
    = \operatorname{Var}[\hat{\theta}_{MoM}]
    + \mathbb{B}^2[\hat{\theta}_{MoM}]
    =\frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}
  \]

* **Consistencia**
  
  Observemos que, por ley fuerte de los grandes números:
  
  \[
  \hat{p}_{MoM} = \frac{T_{per}}{n} \xrightarrow{cs} p
  \]
  
  Luego, como $\hat{\theta}_{MoM}$ es una función continua de $\hat{p}_{MoM}$:
  
  \[
  \hat{\theta}_{MoM} \xrightarrow{cs} \frac{p + S_{p}-1}{S_{e}+S_{p}-1} = \theta
  \]
  
  Por lo tanto, $\hat{\theta}_{MoM}$ es fuertemente consistente.

(Para estimar estos valores se puede reemplazar $p$ por $\hat{p}_{MoM}$, si $p$ es desconocido.) 

### 2.1.6 ECM del test perfecto y test imprefecto en funcion de n 

* **ECM del test perfecto e imperfecto:**

  Recordemos que el ECM del test perfecto es:

  \[
  \text{ECM}(\hat{\theta}_{per})
    = \frac{\theta(1-\theta)}{n}.
  \]

  Y el ECM del tets imperfecto es:

 
  \[
  \text{ECM}(\hat{\theta}_{MoM})
    = \frac{p(1-p)}{n(S_{e}+S_{p}-1)^2}
  \]



Podemos ver que el ECM del tets perfecto es mas chico que el del test imperfecto gracias a un factor:

  \[
  \frac{1}{(S_{e}+S_{p}-1)^2}
  \]
  
Lo que hace que con los valores anteriormente usados, $Se = 0.9$, $Sp = 0.95$; 

$$
Se + Sp - 1 = 0.85
$$
Por lo tanto, 

  \[
  \frac{1}{0.85^2} ≈ 1.38
  \]


Osea que **el ECM del test imperfecto es 38%  mas grande** que el del test perfecto.

```{r}

# Parámetros
tita <- 0.25
Se <- 0.9
Sp <- 0.95

# p real del test imperfecto
p <- (Se + Sp - 1)*tita + (1 - Sp)

# Rango de n
ns <- 10:1000

# ECM del test perfecto
ECM_perfecto <- tita*(1 - tita) / ns

# ECM del test imperfecto 
ECM_imperfecto <- p*(1 - p) / (ns * (Se + Sp - 1)^2)

# Gráfico
plot(ns, ECM_imperfecto,
     type="l", col="orange", lwd=2,
     xlab="n", ylab="ECM",
     main="ECM vs n: Test perfecto vs imperfecto")

lines(ns, ECM_perfecto, col="blue", lwd=2)

legend("topright",
       legend=c("ECM test imperfecto", "ECM test perfecto"),
       col=c("orange", "blue"), lwd=2)

```

Podemos ver que para todos los tamaños muestrales n, el ECM del test imperfecto es mayor que el del test perfecto.
A medida que n crece, ambos ECM tienden a 0, pero el imperfecto siempre queda más arriba que el perfecto.
Cuanto más pequeño sea $Se+Sp−1$ (es decir, cuanto peor sea el test), mayor será la brecha entre las dos curvas.


### 2.1.7 Comparo valores teoricos hallados con los simulados
